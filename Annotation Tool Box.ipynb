{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'git' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Soundboy007/MaskRCNN_training.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -- coding: utf-8 --\n",
    "import cv2\n",
    "import datetime\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random as r\n",
    "import skimage.io as io\n",
    "import string\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import transforms as T\n",
    "import utils\n",
    "from engine import train_one_epoch, evaluate\n",
    "from PIL import Image\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from skimage.util import random_noise\n",
    "from skimage.filters import gaussian\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "            \n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"images\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"masks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"masks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "class AnnotationToolBox(object):\n",
    "\n",
    "    '''\n",
    "    ANNOTATION CLASS with following functionalities:\n",
    "    (a) Training: modelTraining;\n",
    "    (b) Inference: imageMask, dirMask; \n",
    "    (c) Data Augmentation: transformImage, transformDir, rotateImage, AffineImage, flipLRImage, flipUDImage, \n",
    "        noiseImage, blurImage, zoomImage, dullImage, brightImage, and hueImage;\n",
    "    (d) Video to Frames: video2frames.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.device = (torch.device('cpu') if torch.cuda.is_available() else torch.device('cpu'))  # Using GPU if available\n",
    "\n",
    "    \n",
    "    def modelTraining(self, root):\n",
    "        \"\"\"Function that returns a trained model on given dataset\n",
    "\n",
    "        Input: input a folderPath that contains Images and corresponding Masks in two seperate folders. Please ensure that there are only image files in both the folders\n",
    "        Output: a model trained for 10 epochs and saved as 'model(current time).pth' in the current directory\n",
    "        root\n",
    "            --images\n",
    "                    --1.png\n",
    "                    ...\n",
    "            --masks\n",
    "                    --1_mask.png\n",
    "                    ...\n",
    "\n",
    "        Typical usage example:\n",
    "\n",
    "        obj = AnnotationToolBox()\n",
    "        model = obj.modelTraining('dataset/PenFudenDataset')\n",
    "        \"\"\"\n",
    "\n",
    "        # cloning git files containing helper functions for the class\n",
    "#         !cd $root\n",
    "#         !git clone https://github.com/Soundboy007/MaskRCNN_training.git\n",
    "#         !cp MaskRCNN_training/* ../\n",
    "#         !rm -rf MaskRCNN_training\n",
    "\n",
    "        for name in glob.glob(root + '/masks/*'):\n",
    "            mask = Image.open(name)\n",
    "            mask = np.array(mask)\n",
    "            (_, mask) = cv2.threshold(mask, 0, 0XFF, cv2.THRESH_BINARY)\n",
    "            mask //= 255\n",
    "            cv2.imwrite(name, mask)\n",
    "\n",
    "        dataset = Dataset(root)\n",
    "\n",
    "        def get_instance_segmentation_model(num_classes):\n",
    "            # load an instance segmentation model pre-trained on COCO\n",
    "            model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "            # get the number of input features for the classifier\n",
    "            in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "            # replace the pre-trained head with a new one\n",
    "            model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "            # now get the number of input features for the mask classifier\n",
    "            in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "            hidden_layer = 256\n",
    "            # and replace the mask predictor with a new one\n",
    "            model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                               hidden_layer,\n",
    "                                                               num_classes)\n",
    "\n",
    "            return model\n",
    "\n",
    "        def get_transform(train):\n",
    "            transforms = []\n",
    "            # converts the image, a PIL image, into a PyTorch Tensor\n",
    "            transforms.append(T.ToTensor())\n",
    "            if train:\n",
    "                # during training, randomly flip the training images\n",
    "                # and ground-truth for data augmentation\n",
    "                transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "            return T.Compose(transforms)\n",
    "\n",
    "        # use our dataset and defined transformations\n",
    "        dataset = Dataset(root, get_transform(train=True))\n",
    "        dataset_test = Dataset(root, get_transform(train=False))\n",
    "\n",
    "        # split the dataset in train and test set\n",
    "        torch.manual_seed(1)\n",
    "        indices = torch.randperm(len(dataset)).tolist()\n",
    "        dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "        dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "        # define training and validation data loaders\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=2, shuffle=True, num_workers=0,\n",
    "            collate_fn=utils.collate_fn)\n",
    "\n",
    "        data_loader_test = torch.utils.data.DataLoader(\n",
    "            dataset_test, batch_size=1, shuffle=False, num_workers=0,\n",
    "            collate_fn=utils.collate_fn)\n",
    "\n",
    "        # our dataset has two classes only - background and person\n",
    "        num_classes = 2\n",
    "\n",
    "        # get the model using our helper function\n",
    "        model = get_instance_segmentation_model(num_classes)\n",
    "        # move model to the right device\n",
    "        model.to(self.device)\n",
    "\n",
    "        # construct an optimizer\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                    momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "        # and a learning rate scheduler which decreases the learning rate by\n",
    "        # 10x every 3 epochs\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                       step_size=3,\n",
    "                                                       gamma=0.1)\n",
    "\n",
    "        # let's train it for 10 epochs\n",
    "        num_epochs = 1\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # train for one epoch, printing every 10 iterations\n",
    "            train_one_epoch(model, optimizer, data_loader, self.device, epoch, print_freq=10)\n",
    "            # update the learning rate\n",
    "            lr_scheduler.step()\n",
    "            # evaluate on the test dataset\n",
    "            evaluate(model, data_loader_test, device=self.device)\n",
    "            \n",
    "#         !cd ../\n",
    "#         !rm *\n",
    "\n",
    "        torch.save(model, 'model' + str(datetime.datetime.now().time()) + '.pth')\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def imageMask(self, modelPath, imagePath):\n",
    "        \"\"\"\n",
    "        Returns Mask of the image using given model\n",
    "        Input: Path of the model, Path of the image\n",
    "        Output: Stores Mask alongside the image\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            model = torch.load(modelPath, map_location=self.device)  # Reading the model from path provided\n",
    "            image = cv2.imread(imagePath)  # Reading the image from path provided\n",
    "\n",
    "            background = np.zeros(image.shape[:-1])  # Storing background image for mask\n",
    "\n",
    "            image = ToTensor()(image)\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                prediction = model([image.to(self.device)])  # Using model to find human instances in the image\n",
    "\n",
    "            try:\n",
    "                foreground = []\n",
    "                for i in range(len(prediction[0]['masks'])):\n",
    "                    foreground.append(np.array(Image.fromarray(prediction[0]['masks'\n",
    "                            ][i, 0].mul(0XFF).byte().cpu().numpy())))\n",
    "\n",
    "                human_instance = foreground[0]\n",
    "                for i in foreground[1:]:\n",
    "                    human_instance = cv2.add(i, human_instance)  # Adding all human instances into the image\n",
    "            except:\n",
    "\n",
    "                human_instance = background\n",
    "\n",
    "            # thresholding the mask to rid of extra segmentation\n",
    "\n",
    "            human_instance = cv2.merge((human_instance, human_instance,\n",
    "                    human_instance))\n",
    "            (_, mask) = cv2.threshold(human_instance, 100, 0XFF,\n",
    "                    cv2.THRESH_BINARY)\n",
    "\n",
    "            # writing the image mask in the same folder\n",
    "\n",
    "            cv2.imwrite(imagePath.split('.')[0] + '_m.png', mask)\n",
    "\n",
    "            return mask\n",
    "        \n",
    "        except:\n",
    "            None\n",
    "\n",
    "    def dirMask(self, modelPath, folderPath):\n",
    "        \"\"\"\n",
    "        Returns all masks of images present in the folder\n",
    "        Input: Path of model, Path of the folder containing images\n",
    "        Output: Masks of the images stored in the folder\n",
    "        \"\"\"\n",
    "\n",
    "        folderPath += '/*'\n",
    "        for name in glob.glob(folderPath):\n",
    "            print('Writing mask for: ' + name)\n",
    "            self.imageMask(modelPath, name)\n",
    "            \n",
    "    def video2frames(self, videoPath):\n",
    "        \"\"\"\n",
    "        Returns every fifth frame of a video and saves into a folder\n",
    "        Input: path of the video\n",
    "        Output: video frames saved into 'video_path/videoFrames/' folder\n",
    "        \"\"\"\n",
    "\n",
    "        fg_video = cv2.VideoCapture(videoPath)  # foreground video path here\n",
    "        numFrames = 0\n",
    "\n",
    "        path = videoPath.split('.')[0] + '/videoFrames/'\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except OSError:\n",
    "            print('Creation of the directory %s failed' % path)\n",
    "        else:\n",
    "            print('Saving frames into the directory %s' % path)\n",
    "\n",
    "        while fg_video.isOpened():\n",
    "            numFrames += 1\n",
    "            (Dret, Dframe) = fg_video.read()\n",
    "\n",
    "            if Dret == True and numFrames % 5 == 0:\n",
    "\n",
    "                # saving every fifth frame of the video\n",
    "\n",
    "                x = ''.join(r.choice(string.ascii_uppercase\n",
    "                            + string.ascii_lowercase + string.digits)\n",
    "                            for _ in range(16))\n",
    "                cv2.imwrite(path + 'videoFrame_' + x + '.png', Dframe)\n",
    "                \n",
    "            elif Dret == False:\n",
    "                break\n",
    "\n",
    "        fg_video.release()\n",
    "        \n",
    "    def transformImage(self, imagePath, num_transforms=1):\n",
    "        \"\"\"\n",
    "        Returns 'num_transforms' number of images with randomized transforms\n",
    "        Input: Path of image, number of images\n",
    "        Output: Returns images with various transforms applied and stores alongside image\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            if type(imagePath) is str:\n",
    "                image = io.imread(imagePath)\n",
    "            mid_img = image.copy()\n",
    "\n",
    "            for i in range(num_transforms):\n",
    "                if r.choice([1, 0]):  # print('rotate')\n",
    "                    mid_img = self.rotateImage(mid_img)\n",
    "                if r.choice([1, 0]):  # print('flipLR')\n",
    "                    mid_img = self.flipLRImage(mid_img)\n",
    "                if r.choice([1, 0]):  # print('flipUD')\n",
    "                    mid_img = self.flipUDImage(mid_img)\n",
    "                if r.choice([1, 0]):  # print('noise')\n",
    "                    mid_img = self.noiseImage(mid_img)\n",
    "                if r.choice([1, 0]):  # print('blur')\n",
    "                    mid_img = self.blurImage(mid_img)\n",
    "                if r.choice([1, 0]):  # print('hue')\n",
    "                    mid_img = self.hueImage(mid_img)\n",
    "\n",
    "        #         if r.choice([1,0]): mid_img = self.affineImage(mid_img)\n",
    "        #         if r.choice([1,0]): mid_img = self.zoomImage(mid_img)\n",
    "        #         if r.choice([1,0]): mid_img = self.dullImage(mid_img)\n",
    "        #         if r.choice([1,0]): mid_img = self.brightImage(mid_img)\n",
    "\n",
    "                x = ''.join(r.choice(string.ascii_uppercase\n",
    "                            + string.ascii_lowercase + string.digits)\n",
    "                            for _ in range(16))\n",
    "                io.imsave(imagePath.split('.')[0] + '_' + x + '.png',\n",
    "                          mid_img)\n",
    "\n",
    "            return mid_img\n",
    "        \n",
    "        except:\n",
    "            None\n",
    "\n",
    "    def transformDir(self, folderPath, num_transformations=1):\n",
    "        \"\"\"\n",
    "        Generates 'num_transforms' number of images with randomized transforms for all images in the directory\n",
    "        Input: Path of folder containing images, num_transformation (number of randomized pictures for one image)\n",
    "        Output: Returns images with various transforms applied and stores in the folder alongside the images\n",
    "        \"\"\"\n",
    "\n",
    "        folderPath += '/*'\n",
    "        for name in glob.glob(folderPath):\n",
    "            print('Randomizing image: ' + name)\n",
    "            self.transformImage(name, num_transformations)\n",
    "\n",
    "    def rotateImage(self, image, angle = None):\n",
    "        \n",
    "        \"\"\"Function that returns a rotated image for a given image.\n",
    "\n",
    "        A rotation between 0 and 360 degrees is applied randomly. The function accepts a NumPy array or an image path as input.\n",
    "\n",
    "        Typical usage example:\n",
    "\n",
    "        obj = AnnotationToolBox()\n",
    "        bar = obj.rotateImage(image)\n",
    "        test = obj.rotateImage('PNGImages/FudanPed00001.png', 100)\n",
    "        \"\"\"\n",
    "\n",
    "        angle = r.randint(0, 360) if angle is None else angle\n",
    "\n",
    "        if type(image) is str:\n",
    "            image = io.imread(image)\n",
    "            \n",
    "        rotated = rotate(image, angle=angle, mode='wrap')\n",
    "        \n",
    "        return rotated\n",
    "\n",
    "    def affineImage(self, image, translation = None):\n",
    "\n",
    "        \"\"\"Function that returns an affine transform of the input image.\n",
    "\n",
    "        An affine transform is applied to the image, moving the image across the image's height and width. The function accepts a NumPy array as an input image.\n",
    "\n",
    "        Typical usage example:\n",
    "\n",
    "        obj = AnnotationToolBox()\n",
    "        bar = obj.affineImage(image)\n",
    "        test = obj.affineImage('PNGImages/FudanPed00001.png', (100, 50))\n",
    "        \"\"\"\n",
    "        \n",
    "        if type(image) is str:\n",
    "            image = io.imread(image)\n",
    "        \n",
    "        translation = (r.randint(0, image.shape[0]), r.randint(0, image.shape[1])) if translation is None else translation\n",
    "            \n",
    "        transform = AffineTransform(translation=translation)\n",
    "        wrapShift = warp(image, transform, mode='wrap')\n",
    "        \n",
    "        return wrapShift\n",
    "\n",
    "    def flipLRImage(self, image):\n",
    "\n",
    "        \"\"\"Function that returns a Left-Right flipped image for a given image. The function accepts a NumPy array as an input image.\n",
    "\n",
    "        Typical usage example:\n",
    "\n",
    "        obj = AnnotationToolBox()\n",
    "        bar = obj.filpLRImage(image)\n",
    "        \"\"\"\n",
    "\n",
    "        if type(image) is str:\n",
    "            image = io.imread(image)\n",
    "            \n",
    "        flipLR = np.fliplr(image)\n",
    "        \n",
    "        return flipLR\n",
    "\n",
    "    def flipUDImage(self, image):\n",
    "\n",
    "        \"\"\"Function that returns a flipped upside down image for a given image. The function accepts a NumPy array as an input image.\n",
    "\n",
    "        Typical usage example:\n",
    "\n",
    "        obj = AnnotationToolBox()\n",
    "        bar = obj.flipUDImage(image)\n",
    "        \"\"\"\n",
    "\n",
    "        if type(image) is str:\n",
    "            image = io.imread(image)\n",
    "            \n",
    "        flipUD = np.flipud(image)\n",
    "        \n",
    "        return flipUD\n",
    "\n",
    "    def noiseImage(self, image, sigma = None):\n",
    "\n",
    "        \"\"\"Function that returns the input image with added Gaussian noise, with sigma = (0, 0.3) (standard deviation ranging (0, 0.09)). The function accepts a NumPy array as an input image.\n",
    "\n",
    "        Typical usage example:\n",
    "\n",
    "        obj = AnnotationToolBox()\n",
    "        bar = obj.noiseImage(image)\n",
    "        test = obj.noiseImage('PNGImages/FudanPed00001.png', 0.15)\n",
    "        \"\"\"\n",
    "\n",
    "        sigma = r.uniform(0, 0.3) if sigma is None else sigma\n",
    "        \n",
    "        if type(image) is str:\n",
    "            image = io.imread(image)\n",
    "            \n",
    "        noisyRandom = random_noise(image, var=sigma ** 2)\n",
    "        \n",
    "        return noisyRandom\n",
    "\n",
    "    def blurImage(self, image, sigma = None):\n",
    "        \n",
    "        \"\"\"Function that returns the input image with added Gaussian blur (values = (0, 2)). The function accepts a NumPy array as an input image.\n",
    "\n",
    "        Typical usage example:\n",
    "\n",
    "        obj = AnnotationToolBox()\n",
    "        bar = obj.blurImage(image)\n",
    "        test = obj.blurImage('PNGImages/FudanPed00001.png', 1.5)\n",
    "        \"\"\"\n",
    "\n",
    "        sigma = r.uniform(0, 2) if sigma is None else sigma\n",
    "        \n",
    "        if type(image) is str:\n",
    "            image = io.imread(image)\n",
    "            \n",
    "        blurred = gaussian(image, sigma=sigma,\n",
    "                           multichannel=True)\n",
    "        \n",
    "        return blurred\n",
    "\n",
    "    def zoomImage(self, image):\n",
    "\n",
    "        \"\"\"Function that returns a zoomed image of the input image.\n",
    "\n",
    "        A random crop of the image is resized to the original size of the image. The function accepts a NumPy array as an input image.\n",
    "\n",
    "        Typical usage example:\n",
    "\n",
    "        obj = Annotation()\n",
    "        bar = obj.zoomImage(image)\n",
    "        \"\"\"\n",
    "        \n",
    "        if type(image) is str:\n",
    "            image = io.imread(image)\n",
    "            \n",
    "        (x1, x2) = (r.randint(0, image.shape[0] // 2),\n",
    "                    r.randint(image.shape[0] // 2, image.shape[0]))\n",
    "        (y1, y2) = (r.randint(0, image.shape[1] // 2),\n",
    "                    r.randint(image.shape[1] // 2, image.shape[1]))\n",
    "        zoomed = cv2.resize(image[x1:x2, y1:y2], (image.shape[0],\n",
    "                            image.shape[1]), cv2.INTER_LINEAR)\n",
    "        \n",
    "        return zoomed\n",
    "\n",
    "    def dullImage(self, image, degree = None):\n",
    "\n",
    "        \"\"\"Function that returns a darker image of the input image. The function accepts a NumPy array as an input image.\n",
    "\n",
    "        Typical usage example:\n",
    "\n",
    "        obj = AnnotationToolBox()\n",
    "        bar = obj.dullImage(image)\n",
    "        test = obj.dullImage('PNGImages/FudanPed00001.png', 80)\n",
    "        \"\"\"\n",
    "\n",
    "        degree = r.randint(0, 100) if degree is None else degree\n",
    "\n",
    "        if type(image) is str:\n",
    "            image = io.imread(image)\n",
    "            \n",
    "        dullImage = cv2.subtract(image, np.ones_like(image)\n",
    "                                 * degree)\n",
    "        \n",
    "        return dullImage\n",
    "\n",
    "    def brightImage(self, image, degree = None):\n",
    "\n",
    "        \"\"\"Function that returns a lighter image of the input image. The function accepts a NumPy array as an input image.\n",
    "\n",
    "        Typical usage example:\n",
    "\n",
    "        obj = AnnotationToolBox()\n",
    "        bar = obj.brightImage(image)\n",
    "        test = obj.brightImage('PNGImages/FudanPed00001.png', 80)\n",
    "        \"\"\"\n",
    "        \n",
    "        degree = r.randint(0, 100) if degree is None else degree\n",
    "\n",
    "        if type(image) is str:\n",
    "            image = io.imread(image)\n",
    "            \n",
    "        brightImage = cv2.add(image, np.ones_like(image) * degree)\n",
    "        \n",
    "        return brightImage\n",
    "\n",
    "    def hueImage(self, image):\n",
    "\n",
    "        \"\"\"Function that returns the input image with a changed hue.\n",
    "\n",
    "        The [R, G, B] values are flipped to create [B, G, R] pixels to create a hue transform. The function accepts a NumPy array as an input image.\n",
    "\n",
    "        Typical usage example:\n",
    "\n",
    "        obj = Annotation()\n",
    "        bar = obj.brightImage(image)\n",
    "        \"\"\"\n",
    "\n",
    "        if type(image) is str:\n",
    "            image = io.imread(image)\n",
    "            \n",
    "        hueChange = image[..., ::-1]\n",
    "        \n",
    "        return hueChange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
